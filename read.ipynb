{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d28b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using device: cuda\n",
      "[INFO] 전체 샘플 수: 4078789\n",
      "[INFO] Train Samples: 3263031, Test Samples: 815758\n",
      "[INFO] Vocab Size: 73\n",
      "\n",
      "==================================================\n",
      " [INFO] Training Model: LSTM\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 3187/3187 [04:51<00:00, 10.93it/s, loss=0.396]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Avg Loss: 0.4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 3187/3187 [08:49<00:00,  6.02it/s, loss=0.361]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Avg Loss: 0.3879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 3187/3187 [07:54<00:00,  6.72it/s, loss=0.349]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Avg Loss: 0.3840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 3187/3187 [09:30<00:00,  5.58it/s, loss=0.366]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Avg Loss: 0.3824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 3187/3187 [05:27<00:00,  9.73it/s, loss=0.349]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Avg Loss: 0.3807\n",
      "\n",
      "[INFO] Evaluating LSTM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 797/797 [01:11<00:00, 11.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " >>> LSTM Detailed Classification Report <<<\n",
      "\n",
      "[Confusion Matrix]\n",
      "TN (정상->정상): 589351 \t FP (정상->공격/오탐): 17372\n",
      "FN (공격->정상/미탐): 105739 \t TP (공격->공격/정탐): 103296\n",
      "----------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Normal     0.8479    0.9714    0.9054    606723\n",
      "      Attack     0.8560    0.4942    0.6266    209035\n",
      "\n",
      "    accuracy                         0.8491    815758\n",
      "   macro avg     0.8520    0.7328    0.7660    815758\n",
      "weighted avg     0.8500    0.8491    0.8340    815758\n",
      "\n",
      "\n",
      "==================================================\n",
      " [INFO] Training Model: CNN+LSTM\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 3187/3187 [09:22<00:00,  5.67it/s, loss=0.413]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Avg Loss: 0.3929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 3187/3187 [06:30<00:00,  8.16it/s, loss=0.349]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Avg Loss: 0.3814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 3187/3187 [05:15<00:00, 10.11it/s, loss=0.369]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Avg Loss: 0.3793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 3187/3187 [05:18<00:00, 10.00it/s, loss=0.369]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Avg Loss: 0.3780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 3187/3187 [05:12<00:00, 10.19it/s, loss=0.366]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Avg Loss: 0.3770\n",
      "\n",
      "[INFO] Evaluating CNN+LSTM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 797/797 [01:10<00:00, 11.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " >>> CNN+LSTM Detailed Classification Report <<<\n",
      "\n",
      "[Confusion Matrix]\n",
      "TN (정상->정상): 589035 \t FP (정상->공격/오탐): 17688\n",
      "FN (공격->정상/미탐): 103049 \t TP (공격->공격/정탐): 105986\n",
      "----------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Normal     0.8511    0.9708    0.9070    606723\n",
      "      Attack     0.8570    0.5070    0.6371    209035\n",
      "\n",
      "    accuracy                         0.8520    815758\n",
      "   macro avg     0.8540    0.7389    0.7721    815758\n",
      "weighted avg     0.8526    0.8520    0.8379    815758\n",
      "\n",
      "\n",
      "==================================================\n",
      " [INFO] Training Model: Bi-LSTM\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 3187/3187 [05:25<00:00,  9.81it/s, loss=0.398]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Avg Loss: 0.4038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 3187/3187 [05:23<00:00,  9.86it/s, loss=0.388]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Avg Loss: 0.3865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 3187/3187 [05:26<00:00,  9.76it/s, loss=0.381]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Avg Loss: 0.3829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 3187/3187 [07:50<00:00,  6.78it/s, loss=0.357]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Avg Loss: 0.3807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 3187/3187 [06:59<00:00,  7.60it/s, loss=0.391]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Avg Loss: 0.3795\n",
      "\n",
      "[INFO] Evaluating Bi-LSTM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 797/797 [01:17<00:00, 10.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " >>> Bi-LSTM Detailed Classification Report <<<\n",
      "\n",
      "[Confusion Matrix]\n",
      "TN (정상->정상): 594793 \t FP (정상->공격/오탐): 11930\n",
      "FN (공격->정상/미탐): 109981 \t TP (공격->공격/정탐): 99054\n",
      "----------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Normal     0.8439    0.9803    0.9070    606723\n",
      "      Attack     0.8925    0.4739    0.6191    209035\n",
      "\n",
      "    accuracy                         0.8506    815758\n",
      "   macro avg     0.8682    0.7271    0.7630    815758\n",
      "weighted avg     0.8564    0.8506    0.8332    815758\n",
      "\n",
      "\n",
      "==================================================\n",
      " [Final F1-Score Comparison]\n",
      "==================================================\n",
      "LSTM: 0.6266\n",
      "CNN-LSTM: 0.6371\n",
      "Bi-LSTM: 0.6191\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ==========================================\n",
    "# 1. 설정 (속도 최적화 적용)\n",
    "# ==========================================\n",
    "class Config:\n",
    "    CSV_PATH = \"LID-DS-2021_Seq2Seq_Dataset.csv\"\n",
    "    MAX_LEN = 50\n",
    "    BATCH_SIZE = 1024\n",
    "    NUM_WORKERS = 0       # 데이터 로딩 병렬 처리\n",
    "    EMBED_DIM = 64\n",
    "    HIDDEN_DIM = 128\n",
    "    EPOCHS = 5\n",
    "    LR = 0.001\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. 데이터셋\n",
    "# ==========================================\n",
    "class ComparisonDataset(Dataset):\n",
    "    def __init__(self, csv_file, max_len=50):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        # NaN 처리 및 문자열 변환\n",
    "        self.df['input_sequence'] = self.df['input_sequence'].fillna(\"\").astype(str)\n",
    "        \n",
    "        # 단어 사전 구축\n",
    "        self.vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "        self.build_vocab()\n",
    "        \n",
    "    def build_vocab(self):\n",
    "        idx = 2\n",
    "        for seq in self.df['input_sequence']:\n",
    "            for word in seq.split():\n",
    "                if word not in self.vocab:\n",
    "                    self.vocab[word] = idx\n",
    "                    idx += 1\n",
    "                    \n",
    "    def text_to_idx(self, text):\n",
    "        return [self.vocab.get(w, 1) for w in str(text).split()]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "        text = row['input_sequence']\n",
    "        label = row['label'] # 0 or 1\n",
    "        \n",
    "        indices = self.text_to_idx(text)\n",
    "        \n",
    "        # Padding\n",
    "        if len(indices) < self.max_len:\n",
    "            indices += [0] * (self.max_len - len(indices))\n",
    "        else:\n",
    "            indices = indices[:self.max_len]\n",
    "            \n",
    "        return torch.tensor(indices), torch.tensor(label, dtype=torch.float)\n",
    "\n",
    "# ==========================================\n",
    "# 3. 모델 정의 (3가지 종류)\n",
    "# ==========================================\n",
    "\n",
    "# Model 1: Standard LSTM\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hid_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, hid_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hid_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        _, (hidden, _) = self.lstm(embedded)\n",
    "        return self.sigmoid(self.fc(hidden[-1]))\n",
    "\n",
    "# Model 2: CNN + LSTM\n",
    "class CNNLSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hid_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.conv = nn.Conv1d(in_channels=emb_dim, out_channels=hid_dim, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
    "        self.lstm = nn.LSTM(hid_dim, hid_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hid_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x) # (batch, seq, emb)\n",
    "        embedded = embedded.permute(0, 2, 1) # (batch, emb, seq) -> Conv1d input\n",
    "        cnn_out = self.pool(self.relu(self.conv(embedded)))\n",
    "        cnn_out = cnn_out.permute(0, 2, 1) # (batch, seq/2, hid) -> LSTM input\n",
    "        _, (hidden, _) = self.lstm(cnn_out)\n",
    "        return self.sigmoid(self.fc(hidden[-1]))\n",
    "\n",
    "# Model 3: Bi-Directional LSTM\n",
    "class BiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hid_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, hid_dim, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hid_dim * 2, 1) \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        _, (hidden, _) = self.lstm(embedded)\n",
    "        hidden_cat = torch.cat((hidden[-2], hidden[-1]), dim=1)\n",
    "        return self.sigmoid(self.fc(hidden_cat))\n",
    "\n",
    "# ==========================================\n",
    "# 4. 학습 및 평가 함수 (상세 지표 추가)\n",
    "# ==========================================\n",
    "def train_and_evaluate(model_name, model, train_loader, test_loader, device):\n",
    "    print(f\"\\n\" + \"=\"*50)\n",
    "    print(f\" [INFO] Training Model: {model_name}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=Config.LR)\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    # --- Training Phase ---\n",
    "    model.train()\n",
    "    for epoch in range(Config.EPOCHS):\n",
    "        total_loss = 0\n",
    "        loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{Config.EPOCHS}\")\n",
    "        for inputs, labels in loop:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "        \n",
    "    # --- Evaluation Phase ---\n",
    "    print(f\"\\n[INFO] Evaluating {model_name}...\")\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(test_loader, desc=\"Testing\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs).squeeze()\n",
    "            preds = (outputs > 0.5).float()\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "    # --- Detailed Metrics ---\n",
    "    print(f\"\\n >>> {model_name} Detailed Classification Report <<<\")\n",
    "    \n",
    "    # 1. Confusion Matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    print(f\"\\n[Confusion Matrix]\")\n",
    "    print(f\"TN (정상->정상): {tn} \\t FP (정상->공격/오탐): {fp}\")\n",
    "    print(f\"FN (공격->정상/미탐): {fn} \\t TP (공격->공격/정탐): {tp}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # 2. Classification Report\n",
    "    print(classification_report(all_labels, all_preds, target_names=['Normal', 'Attack'], digits=4))\n",
    "    \n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    return f1\n",
    "\n",
    "# ==========================================\n",
    "# 5. 메인 실행\n",
    "# ==========================================\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. 데이터 로드\n",
    "    try:\n",
    "        dataset = ComparisonDataset(Config.CSV_PATH)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"[ERROR] {Config.CSV_PATH} 파일이 없습니다.\")\n",
    "        exit()\n",
    "    \n",
    "    # 데이터셋 분할 (Train 80% / Test 20%)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "    \n",
    "    # DataLoader 최적화 (Batch Size 증가, Num Workers, Pin Memory)\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=Config.BATCH_SIZE, \n",
    "        shuffle=True, \n",
    "        num_workers=Config.NUM_WORKERS, \n",
    "        pin_memory=True\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=Config.BATCH_SIZE, \n",
    "        shuffle=False, \n",
    "        num_workers=Config.NUM_WORKERS, \n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    vocab_size = len(dataset.vocab) + 1\n",
    "    print(f\"[INFO] Vocab Size: {vocab_size}\")\n",
    "    print(f\"[INFO] Train Samples: {len(train_dataset)}, Test Samples: {len(test_dataset)}\")\n",
    "    \n",
    "    # 2. 모델 초기화\n",
    "    model_lstm = LSTMClassifier(vocab_size, Config.EMBED_DIM, Config.HIDDEN_DIM).to(Config.DEVICE)\n",
    "    model_cnn_lstm = CNNLSTMClassifier(vocab_size, Config.EMBED_DIM, Config.HIDDEN_DIM).to(Config.DEVICE)\n",
    "    model_bilstm = BiLSTMClassifier(vocab_size, Config.EMBED_DIM, Config.HIDDEN_DIM).to(Config.DEVICE)\n",
    "    \n",
    "    # 3. 학습 및 비교 실행\n",
    "    results = {}\n",
    "    results['LSTM'] = train_and_evaluate(\"LSTM\", model_lstm, train_loader, test_loader, Config.DEVICE)\n",
    "    results['CNN-LSTM'] = train_and_evaluate(\"CNN+LSTM\", model_cnn_lstm, train_loader, test_loader, Config.DEVICE)\n",
    "    results['Bi-LSTM'] = train_and_evaluate(\"Bi-LSTM\", model_bilstm, train_loader, test_loader, Config.DEVICE)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\" [Final F1-Score Comparison]\")\n",
    "    print(\"=\"*50)\n",
    "    for name, score in results.items():\n",
    "        print(f\"{name}: {score:.4f}\")\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125e093d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 1. Data Loading...\n",
      "Vocab Size: 75\n",
      "Vocab Size: 75\n",
      "[INFO] 2. Model Initialization...\n",
      "\n",
      "[INFO] 3. Starting Pre-training (Supervised)...\n",
      "[INFO] 2. Model Initialization...\n",
      "\n",
      "[INFO] 3. Starting Pre-training (Supervised)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pre-train Epoch 1: 100%|██████████| 3187/3187 [12:52<00:00,  4.13it/s]\n",
      "Pre-train Epoch 1: 100%|██████████| 3187/3187 [12:52<00:00,  4.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-train Loss: 0.5761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pre-train Epoch 2: 100%|██████████| 3187/3187 [16:04<00:00,  3.30it/s]\n",
      "Pre-train Epoch 2: 100%|██████████| 3187/3187 [16:04<00:00,  3.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-train Loss: 0.3019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pre-train Epoch 3: 100%|██████████| 3187/3187 [15:29<00:00,  3.43it/s]\n",
      "Pre-train Epoch 3: 100%|██████████| 3187/3187 [15:29<00:00,  3.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-train Loss: 0.2501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pre-train Epoch 4:  11%|█▏        | 360/3187 [01:27<11:19,  4.16it/s]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, classification_report, precision_score, recall_score\n",
    "\n",
    "# ==========================================\n",
    "# 1. 설정 및 하이퍼파라미터\n",
    "# ==========================================\n",
    "class Config:\n",
    "    CSV_PATH = \"LID-DS-2021_Seq2Seq_Dataset.csv\"  # 전처리된 데이터셋 경로\n",
    "    MAX_LEN = 50          # 시퀀스 최대 길이\n",
    "    BATCH_SIZE = 1024     \n",
    "    NUM_WORKERS = 0       # 데이터 로딩 병렬 처리 개수 (CPU 코어 수에 맞춰 조절 가능)\n",
    "    \n",
    "    EMBED_DIM = 64\n",
    "    HIDDEN_DIM = 128\n",
    "    \n",
    "    # 학습 파이프라인 설정\n",
    "    PRETRAIN_EPOCHS = 5   # 지도학습 에폭 수\n",
    "    RL_EPOCHS = 5         # 강화학습 에폭 수\n",
    "    LR = 0.001\n",
    "    \n",
    "    # RL 파라미터 (논문 참조)\n",
    "    ALPHA = 0.5           # Custom Loss 가중치\n",
    "    GAMMA = 0.99          # Discount Factor\n",
    "    \n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. 데이터셋 및 단어 사전 (공통)\n",
    "# ==========================================\n",
    "class Vocabulary:\n",
    "    def __init__(self):\n",
    "        self.stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
    "        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
    "        self.freq_threshold = 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.stoi)\n",
    "\n",
    "    def build_vocabulary(self, sentence_list):\n",
    "        frequencies = Counter()\n",
    "        idx = 4\n",
    "        for sentence in sentence_list:\n",
    "            if not isinstance(sentence, str): continue\n",
    "            for word in sentence.split():\n",
    "                frequencies[word] += 1\n",
    "        \n",
    "        for word, count in frequencies.items():\n",
    "            if count >= self.freq_threshold:\n",
    "                self.stoi[word] = idx\n",
    "                self.itos[idx] = word\n",
    "                idx += 1\n",
    "\n",
    "    def numericalize(self, text):\n",
    "        tokenized_text = str(text).split()\n",
    "        return [self.stoi.get(token, self.stoi[\"<UNK>\"]) for token in tokenized_text]\n",
    "\n",
    "class LIDDSDataset(Dataset):\n",
    "    def __init__(self, csv_file, vocab):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.vocab = vocab\n",
    "        self.max_len = Config.MAX_LEN\n",
    "        # 문자열 변환 및 필터링\n",
    "        self.df['input_sequence'] = self.df['input_sequence'].fillna(\"\").astype(str)\n",
    "        self.df['target_keywords'] = self.df['target_keywords'].fillna(\"\").astype(str)\n",
    "        self.df = self.df[self.df['input_sequence'].str.len() > 0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "        input_text = row['input_sequence']\n",
    "        target_text = row['target_keywords']\n",
    "        label = row['label'] # 0 (정상) or 1 (공격)\n",
    "\n",
    "        input_indices = self.vocab.numericalize(input_text)\n",
    "        target_indices = [self.vocab.stoi[\"<SOS>\"]] + self.vocab.numericalize(target_text) + [self.vocab.stoi[\"<EOS>\"]]\n",
    "\n",
    "        # Padding\n",
    "        input_indices = self.pad_seq(input_indices)\n",
    "        target_indices = self.pad_seq(target_indices)\n",
    "\n",
    "        return (torch.tensor(input_indices), \n",
    "                torch.tensor(target_indices), \n",
    "                torch.tensor(label, dtype=torch.float))\n",
    "\n",
    "    def pad_seq(self, seq):\n",
    "        if len(seq) < self.max_len:\n",
    "            return seq + [self.vocab.stoi[\"<PAD>\"]] * (self.max_len - len(seq))\n",
    "        return seq[:self.max_len]\n",
    "\n",
    "# ==========================================\n",
    "# 3. 모델 정의 (Seq2Seq with Attention)\n",
    "# ==========================================\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, batch_first=True)\n",
    "\n",
    "    def forward(self, src):\n",
    "        embedded = self.embedding(src)\n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        return outputs, hidden, cell\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hid_dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(hid_dim * 2, hid_dim)\n",
    "        self.v = nn.Linear(hid_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # hidden: [1, batch, hid] -> [batch, seq, hid]\n",
    "        src_len = encoder_outputs.shape[1]\n",
    "        hidden = hidden.permute(1, 0, 2).repeat(1, src_len, 1)\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        return torch.softmax(attention, dim=1)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, attention):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.attention = attention\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn = nn.LSTM(hid_dim + emb_dim, hid_dim, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hid_dim * 2 + emb_dim, output_dim)\n",
    "\n",
    "    def forward(self, input, hidden, cell, encoder_outputs):\n",
    "        input = input.unsqueeze(1)\n",
    "        embedded = self.embedding(input)\n",
    "        a = self.attention(hidden[-1].unsqueeze(0), encoder_outputs).unsqueeze(1)\n",
    "        weighted = torch.bmm(a, encoder_outputs)\n",
    "        rnn_input = torch.cat((embedded, weighted), dim=2)\n",
    "        output, (hidden, cell) = self.rnn(rnn_input, (hidden, cell))\n",
    "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim=2))\n",
    "        return prediction.squeeze(1), hidden, cell\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        batch_size = src.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n",
    "        encoder_outputs, hidden, cell = self.encoder(src)\n",
    "        input = trg[:, 0]\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell, encoder_outputs)\n",
    "            outputs[:, t] = output\n",
    "            top1 = output.argmax(1)\n",
    "            input = trg[:, t] if random.random() < teacher_forcing_ratio else top1\n",
    "            \n",
    "        return outputs\n",
    "\n",
    "# ==========================================\n",
    "# 4. 강화학습 (RL) 로직 & 보상 함수\n",
    "# ==========================================\n",
    "def calculate_reward(pred_indices, target_indices, actual_label, vocab):\n",
    "    \"\"\"\n",
    "    논문 Equation 19 구현: R = (0.3*r1) + (0.5*r2) - (0.2*r3)\n",
    "    - r1: Pre-trained 모델과의 유사도 (여기서는 Target Keyword와의 일치율로 근사)\n",
    "    - r2: 공격 탐지율 (Detection Rate)\n",
    "    - r3: 오탐율 (False Positive)\n",
    "    \"\"\"\n",
    "    batch_size = pred_indices.shape[0]\n",
    "    rewards = []\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        pred_seq = pred_indices[i]\n",
    "        trg_seq = target_indices[i]\n",
    "        label = actual_label[i].item() # 1: Attack, 0: Normal\n",
    "\n",
    "        # 1. r1: 키워드 정확도 (생성된 키워드가 정답 키워드에 포함되는지)\n",
    "        # 0과 패딩 제외\n",
    "        valid_pred = [p.item() for p in pred_seq if p.item() not in [0, 1, 2]]\n",
    "        valid_trg = [t.item() for t in trg_seq if t.item() not in [0, 1, 2]]\n",
    "        \n",
    "        if len(valid_trg) > 0:\n",
    "            match_count = sum([1 for p in valid_pred if p in valid_trg])\n",
    "            r1 = match_count / len(valid_trg)\n",
    "        else:\n",
    "            r1 = 0.0\n",
    "\n",
    "        # 2. r2 & r3: 가상의 Testbed 결과\n",
    "        # 생성된 키워드가 유의미하면(정답과 50% 이상 일치) 탐지 성공으로 간주\n",
    "        is_detected = (r1 > 0.5) \n",
    "\n",
    "        r2 = 0.0 # True Positive Rate\n",
    "        r3 = 0.0 # False Positive Rate\n",
    "\n",
    "        if label == 1: # 실제 공격일 때\n",
    "            if is_detected:\n",
    "                r2 = 1.0 # 공격을 탐지함\n",
    "            else:\n",
    "                r2 = 0.0 # 공격 놓침\n",
    "        else: # 실제 정상일 때\n",
    "            if is_detected:\n",
    "                r3 = 1.0 # 정상을 공격으로 오탐 (False Positive)\n",
    "            else:\n",
    "                r3 = 0.0 # 정상으로 잘 판단\n",
    "\n",
    "        # 최종 Reward 계산\n",
    "        R = (0.3 * r1) + (0.5 * r2) - (0.2 * r3)\n",
    "        rewards.append(R)\n",
    "\n",
    "    return torch.tensor(rewards).to(Config.DEVICE)\n",
    "\n",
    "def train_rl_step(model, src, trg, label, optimizer, criterion, vocab):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # RL에서는 Teacher Forcing을 끄고 모델이 스스로 생성하게 함\n",
    "    output = model(src, trg, teacher_forcing_ratio=0.0) \n",
    "    \n",
    "    # 예측된 토큰 인덱스 추출\n",
    "    pred_indices = output.argmax(dim=2) # [batch, trg_len]\n",
    "    \n",
    "    # 보상 계산\n",
    "    rewards = calculate_reward(pred_indices, trg, label, vocab) # [batch]\n",
    "    \n",
    "    # Loss 계산 (Base Loss)\n",
    "    output_dim = output.shape[-1]\n",
    "    output_flat = output[:, 1:].reshape(-1, output_dim)\n",
    "    trg_flat = trg[:, 1:].reshape(-1)\n",
    "    base_loss = criterion(output_flat, trg_flat)\n",
    "    \n",
    "    # Custom Loss (Algorithm 2)\n",
    "    # 배치 평균 Reward 사용\n",
    "    mean_reward = rewards.mean()\n",
    "    \n",
    "    # Reward를 0~1 사이로 클리핑 (정규화)\n",
    "    normalized_reward = torch.clamp(mean_reward, 0.0, 1.0)\n",
    "    \n",
    "    # Loss에 Reward 반영: Reward가 높을수록 Loss가 작아지도록 (Gradient를 덜 업데이트하거나 방향 조정)\n",
    "    # 논문 수식: Loss = Base_Loss * (1 - alpha * Reward)\n",
    "    custom_loss = base_loss * (1 - Config.ALPHA * normalized_reward)\n",
    "    \n",
    "    custom_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return custom_loss.item(), mean_reward.item()\n",
    "\n",
    "# ==========================================\n",
    "# 5. 평가 함수 (Evaluation - Detailed)\n",
    "# ==========================================\n",
    "def evaluate_model(model, dataloader, vocab):\n",
    "    model.eval()\n",
    "    all_preds = [] # 1 if detected(attack), 0 if normal\n",
    "    all_labels = [] # Ground Truth Labels\n",
    "    \n",
    "    print(\"[INFO] Evaluating Model...\")\n",
    "    with torch.no_grad():\n",
    "        for src, trg, label in tqdm(dataloader, desc=\"Eval\"):\n",
    "            src, trg = src.to(Config.DEVICE), trg.to(Config.DEVICE)\n",
    "            \n",
    "            output = model(src, trg, teacher_forcing_ratio=0.0)\n",
    "            pred_indices = output.argmax(dim=2)\n",
    "            \n",
    "            # 생성된 키워드를 기반으로 공격 여부 판별 (Rule Matching Simulation)\n",
    "            batch_size = src.shape[0]\n",
    "            for i in range(batch_size):\n",
    "                p_seq = [x.item() for x in pred_indices[i] if x.item() not in [0, 1, 2]]\n",
    "                t_seq = [x.item() for x in trg[i] if x.item() not in [0, 1, 2]]\n",
    "                \n",
    "                # Rule: 생성된 키워드가 정답 키워드 집합과 얼마나 겹치는지?\n",
    "                if len(t_seq) > 0:\n",
    "                    match_ratio = sum([1 for p in p_seq if p in t_seq]) / len(t_seq)\n",
    "                else:\n",
    "                    match_ratio = 0\n",
    "                \n",
    "                # 매칭율이 일정 수준 이상이면 \"공격으로 탐지\" 했다고 가정\n",
    "                detected = 1 if match_ratio > 0.3 else 0 \n",
    "                \n",
    "                all_preds.append(detected)\n",
    "                all_labels.append(label[i].item())\n",
    "    \n",
    "    # --- 상세 지표 계산 및 출력 (Cell 16 양식) ---\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    prec = precision_score(all_labels, all_preds, zero_division=0)\n",
    "    rec = recall_score(all_labels, all_preds, zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_preds, zero_division=0)\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(all_labels, all_preds).ravel()\n",
    "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "    fnr = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "    \n",
    "    print(\"========================================\")\n",
    "    print(f\"Accuracy : {acc:.4f} ({acc*100:.2f}%)\")\n",
    "    print(\"----------------------------------------\")\n",
    "    print(f\"FPR      : {fpr:.4f} ({fpr*100:.2f}%)  # 오탐률\")\n",
    "    print(f\"FNR      : {fnr:.4f} ({fnr*100:.2f}%)  # 미탐률\")\n",
    "    print(\"----------------------------------------\")\n",
    "    print(f\"Recall   : {rec:.4f} ({rec*100:.2f}%)  # 검출률\")\n",
    "    print(f\"Precision: {prec:.4f} ({prec*100:.2f}%)\")\n",
    "    print(f\"F1-score : {f1:.4f}\")\n",
    "    print(\"========================================\")\n",
    "    \n",
    "    return acc, f1\n",
    "\n",
    "# ==========================================\n",
    "# 6. 메인 실행 루프\n",
    "# ==========================================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"[INFO] 1. Data Loading...\")\n",
    "    try:\n",
    "        df = pd.read_csv(Config.CSV_PATH)\n",
    "    except FileNotFoundError:\n",
    "        print(\"[ERROR] CSV 파일이 없습니다. 전처리 코드를 먼저 실행하세요.\")\n",
    "        exit()\n",
    "        \n",
    "    vocab = Vocabulary()\n",
    "    # 입력과 정답 모두를 사용하여 단어장 구축\n",
    "    all_text = df['input_sequence'].fillna(\"\").tolist() + df['target_keywords'].fillna(\"\").tolist()\n",
    "    vocab.build_vocabulary(all_text)\n",
    "    print(f\"Vocab Size: {len(vocab)}\")\n",
    "    \n",
    "    dataset = LIDDSDataset(Config.CSV_PATH, vocab)\n",
    "    # 데이터 분할 (Train/Test)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=Config.BATCH_SIZE, \n",
    "        shuffle=True, \n",
    "        num_workers=Config.NUM_WORKERS, \n",
    "        pin_memory=True\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=Config.BATCH_SIZE, \n",
    "        shuffle=False, \n",
    "        num_workers=Config.NUM_WORKERS, \n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    print(\"[INFO] 2. Model Initialization...\")\n",
    "    enc = Encoder(len(vocab), Config.EMBED_DIM, Config.HIDDEN_DIM)\n",
    "    attn = Attention(Config.HIDDEN_DIM)\n",
    "    dec = Decoder(len(vocab), Config.EMBED_DIM, Config.HIDDEN_DIM, attn)\n",
    "    model = Seq2Seq(enc, dec, Config.DEVICE).to(Config.DEVICE)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=Config.LR)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=vocab.stoi[\"<PAD>\"])\n",
    "    \n",
    "    # ---------------------------------------------------------\n",
    "    # Phase 1: Pre-training (Supervised Learning)\n",
    "    # ---------------------------------------------------------\n",
    "    print(\"\\n[INFO] 3. Starting Pre-training (Supervised)...\")\n",
    "    for epoch in range(Config.PRETRAIN_EPOCHS):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for src, trg, _ in tqdm(train_loader, desc=f\"Pre-train Epoch {epoch+1}\"):\n",
    "            src, trg = src.to(Config.DEVICE), trg.to(Config.DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(src, trg, teacher_forcing_ratio=0.5)\n",
    "            \n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[:, 1:].reshape(-1, output_dim)\n",
    "            trg = trg[:, 1:].reshape(-1)\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        print(f\"Pre-train Loss: {epoch_loss/len(train_loader):.4f}\")\n",
    "        \n",
    "    # ---------------------------------------------------------\n",
    "    # Phase 2: RL Training (Actor-Critic / Policy Gradient)\n",
    "    # ---------------------------------------------------------\n",
    "    print(\"\\n[INFO] 4. Starting RL Training (Reward Based)...\")\n",
    "    for epoch in range(Config.RL_EPOCHS):\n",
    "        total_loss = 0\n",
    "        total_reward = 0\n",
    "        for src, trg, label in tqdm(train_loader, desc=f\"RL Epoch {epoch+1}\"):\n",
    "            src, trg, label = src.to(Config.DEVICE), trg.to(Config.DEVICE), label.to(Config.DEVICE)\n",
    "            loss, reward = train_rl_step(model, src, trg, label, optimizer, criterion, vocab)\n",
    "            total_loss += loss\n",
    "            total_reward += reward\n",
    "            \n",
    "        print(f\"RL Epoch {epoch+1} - Loss: {total_loss/len(train_loader):.4f}, Avg Reward: {total_reward/len(train_loader):.4f}\")\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Phase 3: Final Evaluation\n",
    "    # ---------------------------------------------------------\n",
    "    print(\"\\n[INFO] 5. Final Evaluation...\")\n",
    "    acc, f1 = evaluate_model(model, test_loader, vocab)\n",
    "    \n",
    "    # 모델 저장\n",
    "    torch.save(model.state_dict(), \"rl_hids_model_final.pt\")\n",
    "    print(\"\\n[INFO] Model saved to rl_hids_model_final.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70285aaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
